{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongxiangtranslationproject/dongxiangtranslationproject.github.io/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfb1935fd6396ed7",
      "metadata": {
        "collapsed": false,
        "id": "bfb1935fd6396ed7"
      },
      "source": [
        "## Sentence Import and Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h8RhF7p7iEBC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8RhF7p7iEBC",
        "outputId": "48d9a1f5-b0d1-4c81-e8fe-0bf60fb55beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWZ21T83HwxN",
        "outputId": "f0eecbe6-ad30-483e-b67d-74e0529cefda"
      },
      "id": "oWZ21T83HwxN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_DIR1 = \"/content/drive/MyDrive/my_nllb_DC_model\"\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(MODEL_DIR1)\n",
        "model1 = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR1).to(device)\n",
        "model1.eval()\n",
        "\n",
        "def translate1(text, src_lang=\"sce_Latn\", tgt_lang=\"zho_Hans\",\n",
        "               a=16, b=1.5, max_input_length=1024, **kwargs):\n",
        "    tokenizer1.src_lang = src_lang\n",
        "    inputs = tokenizer1(text, return_tensors=\"pt\", padding=True,\n",
        "                        truncation=True, max_length=max_input_length).to(model1.device)\n",
        "    result = model1.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=tokenizer1.convert_tokens_to_ids(tgt_lang),\n",
        "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
        "        **kwargs\n",
        "    )\n",
        "    outputs = tokenizer1.batch_decode(result, skip_special_tokens=True)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "16lSudPyHhfV"
      },
      "id": "16lSudPyHhfV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_DIR2 = \"/content/drive/MyDrive/my_nllb_DCword_model\"\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(MODEL_DIR2)\n",
        "model2 = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR2).to(device)\n",
        "model2.eval()\n",
        "\n",
        "def translate2(text, src_lang=\"sce_Latn\", tgt_lang=\"zho_Hans\",\n",
        "               a=16, b=1.5, max_input_length=1024, **kwargs):\n",
        "    tokenizer2.src_lang = src_lang\n",
        "    inputs = tokenizer2(text, return_tensors=\"pt\", padding=True,\n",
        "                        truncation=True, max_length=max_input_length).to(model2.device)\n",
        "    result = model2.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=tokenizer2.convert_tokens_to_ids(tgt_lang),\n",
        "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
        "        **kwargs\n",
        "    )\n",
        "    outputs = tokenizer2.batch_decode(result, skip_special_tokens=True)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "hsr51e7R1iJF"
      },
      "id": "hsr51e7R1iJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer = input(\"please enter a Dongxiang sentence:\")\n",
        "if buffer == \"\":\n",
        "  text = \" Chi eqiegvan gaodase\"\n",
        "else:\n",
        "  text = buffer\n",
        "print(\"Source:\", text)\n",
        "print(f\"word-level model Translation: {translate2(text,no_repeat_ngram_size=3, num_beams=5)}\\n\"\n",
        "      f\"sentence-level model Translation: {translate1(text,no_repeat_ngram_size=3, num_beams=5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOTEyo-ZzTeg",
        "outputId": "782e3a4e-30fa-4870-fa88-42e25dc08650"
      },
      "id": "TOTEyo-ZzTeg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "please enter a Dongxiang sentence:\n",
            "Source:  Chi eqiegvan gaodase\n",
            "word-level model Translation: ['你早点好']\n",
            "sentence-level model Translation: ['愿你早日康复']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template: <br>\n",
        "Dongxiang: Chi eqiegvan gaodase <br>\n",
        "Chinese: 愿你早日康复"
      ],
      "metadata": {
        "id": "FOrrWYn34JU7"
      },
      "id": "FOrrWYn34JU7"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_DIR3 = \"/content/drive/MyDrive/my_nllb_CD_model\"\n",
        "tokenizer3 = AutoTokenizer.from_pretrained(MODEL_DIR3)\n",
        "model3 = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR3).to(device)\n",
        "model3.eval()\n",
        "\n",
        "def translate3(text, src_lang=\"zho_Hans\", tgt_lang=\"sce_Latn\",\n",
        "               a=16, b=1.5, max_input_length=1024, **kwargs):\n",
        "    tokenizer3.src_lang = src_lang\n",
        "    inputs = tokenizer3(text, return_tensors=\"pt\", padding=True,\n",
        "                        truncation=True, max_length=max_input_length).to(model1.device)\n",
        "    result = model3.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=tokenizer3.convert_tokens_to_ids(tgt_lang),\n",
        "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
        "        **kwargs\n",
        "    )\n",
        "    outputs = tokenizer3.batch_decode(result, skip_special_tokens=True)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "aBoWRrs5HjrO"
      },
      "id": "aBoWRrs5HjrO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_DIR4 = \"/content/drive/MyDrive/my_nllb_CDword_model\"\n",
        "tokenizer4 = AutoTokenizer.from_pretrained(MODEL_DIR4)\n",
        "model4 = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR4).to(device)\n",
        "model4.eval()\n",
        "\n",
        "def translate4(text, src_lang=\"zho_Hans\", tgt_lang=\"sce_Latn\",\n",
        "               a=16, b=1.5, max_input_length=1024, **kwargs):\n",
        "    tokenizer4.src_lang = src_lang\n",
        "    inputs = tokenizer4(text, return_tensors=\"pt\", padding=True,\n",
        "                        truncation=True, max_length=max_input_length).to(model1.device)\n",
        "    result = model4.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=tokenizer4.convert_tokens_to_ids(tgt_lang),\n",
        "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
        "        **kwargs\n",
        "    )\n",
        "    outputs = tokenizer4.batch_decode(result, skip_special_tokens=True)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "s5Io2J7V6wjM"
      },
      "id": "s5Io2J7V6wjM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer = input(\"please enter a Chinese sentence:\")\n",
        "if buffer == \"\":\n",
        "  text = [\"愿你早日康复\", \"愿你早日康复\"]\n",
        "else:\n",
        "  text = buffer\n",
        "print(\"Source:\", text)\n",
        "print(f\"word-level model Translation: {translate4(text,no_repeat_ngram_size=3, num_beams=5)}\\n\"\n",
        "      f\"sentence-level model Translation: {translate3(text,no_repeat_ngram_size=3, num_beams=5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq68eQy47i2v",
        "outputId": "2e17bffc-33fc-4598-a6ee-d60041e75ba8"
      },
      "id": "Yq68eQy47i2v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "please enter a Chinese sentence:我要买新车\n",
            "Source: 我要买新车\n",
            "word-level model Translation: ['bi shinche agine']\n",
            "sentence-level model Translation: ['bi shini chezi agine giezho']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_bleu_chrf(hyps, refs, tgt_is_zh=False):\n",
        "    bleu_tok = \"zh\" if tgt_is_zh else \"13a\"\n",
        "    bleu = sacrebleu.BLEU(tokenize=bleu_tok)\n",
        "    chrf = sacrebleu.CHRF(word_order=2)\n",
        "\n",
        "    bleu_score = bleu.corpus_score(hyps, [refs])\n",
        "    chrf_score = chrf.corpus_score(hyps, [refs])\n",
        "    return bleu_score, chrf_score"
      ],
      "metadata": {
        "id": "lenMy-hqIUHV"
      },
      "id": "lenMy-hqIUHV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dongxiang = [\"hhe enedu cha ochi\",\n",
        "             \"mayi chezi agi.\",\n",
        "             \"ta ene olon cha izhiewo ye\",\n",
        "             \"magvashijie budan izhiene.\",\n",
        "             \"chi enedu kala echiwo?.\",\n",
        "             \"chi shi ali oron kun?.\",\n",
        "\n",
        "             \"chi khala giemere uzhele echiwo?.\",\n",
        "             \"ene shi mini ochin zhiao.\",\n",
        "             \"Hhe morei unuzhi naduzho.\",\n",
        "             \"Tegha gatashi qianguzhi ijiene.\",\n",
        "             \"Khara naran doura shu uzhese nudunde gao wiwo.\"\n",
        "             ]"
      ],
      "metadata": {
        "id": "wfumx4wYE_OX"
      },
      "id": "wfumx4wYE_OX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Chinese = [\"他今天喝茶\",\n",
        "           \"我们买个车子。\",\n",
        "           \"你们最近吃了吗\",\n",
        "           \"明天吃饭。\",\n",
        "           \"你今天去哪了？\",\n",
        "           \"你是哪里人？\",\n",
        "\n",
        "           \"你去哪看病了？\",\n",
        "           \"这是我的妹妹。\",\n",
        "           \"他骑马玩。\",\n",
        "           \"鸡捡小石子吃。\",\n",
        "           \"烈日下看书的话对眼睛不好。\"\n",
        "           ]"
      ],
      "metadata": {
        "id": "rw2lfobbFFL6"
      },
      "id": "rw2lfobbFFL6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We randomly selected 11 sentences to compute BLEU and ChrF scores. In addition to the two “sentence-level models,” which include artificially generated bilingual sentences, we also trained two “word-level models” that take bilingual word pairs as input instead of full sentences."
      ],
      "metadata": {
        "id": "OdeQl58aRU8c"
      },
      "id": "OdeQl58aRU8c"
    },
    {
      "cell_type": "code",
      "source": [
        "src_dx = dongxiang\n",
        "ref_zh = Chinese\n",
        "\n",
        "hyps_zh = translate1(src_dx, src_lang=\"sou_Latn\", tgt_lang=\"zho_Hans\",no_repeat_ngram_size=3, num_beams=5)\n",
        "bleu_zh, chrf_zh = eval_bleu_chrf(hyps_zh, ref_zh, tgt_is_zh=True)\n",
        "print(\"Sentence[DX→ZH] BLEU:\", bleu_zh)\n",
        "print(\"Sentence[DX→ZH] ChrF++:\", chrf_zh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu2_mK_9ImBt",
        "outputId": "067dd06a-71fa-4fa6-cdbb-f16fbee0ddcb"
      },
      "id": "vu2_mK_9ImBt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence[DX→ZH] BLEU: BLEU = 44.00 70.5/50.7/39.3/26.7 (BP = 1.000 ratio = 1.026 hyp_len = 78 ref_len = 76)\n",
            "Sentence[DX→ZH] ChrF++: chrF2++ = 34.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_dx = dongxiang\n",
        "ref_zh = Chinese\n",
        "\n",
        "hyps_zh = translate2(src_dx, src_lang=\"sou_Latn\", tgt_lang=\"zho_Hans\",no_repeat_ngram_size=3, num_beams=5)\n",
        "bleu_zh, chrf_zh = eval_bleu_chrf(hyps_zh, ref_zh, tgt_is_zh=True)\n",
        "print(\"Word[DX→ZH] BLEU:\", bleu_zh)\n",
        "print(\"Word[DX→ZH] ChrF++:\", chrf_zh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvRy-w6J_vFL",
        "outputId": "928533f1-5569-4f79-cbd2-c9dd4b370369"
      },
      "id": "YvRy-w6J_vFL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word[DX→ZH] BLEU: BLEU = 24.69 50.0/28.4/19.6/13.3 (BP = 1.000 ratio = 1.026 hyp_len = 78 ref_len = 76)\n",
            "Word[DX→ZH] ChrF++: chrF2++ = 20.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_dx = Chinese\n",
        "ref_zh = dongxiang\n",
        "\n",
        "hyps_zh = translate3(src_dx, src_lang=\"zho_Hans\", tgt_lang=\"sou_Latn\",no_repeat_ngram_size=3, num_beams=5)\n",
        "bleu_zh, chrf_zh = eval_bleu_chrf(hyps_zh, ref_zh, tgt_is_zh=True)\n",
        "print(\"Sentence[ZH→DX] BLEU:\", bleu_zh)\n",
        "print(\"Sentence[ZH→DX] ChrF++:\", chrf_zh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8fXScxWAVIk",
        "outputId": "4a60c2a5-ae8f-4b35-c20d-e111511ff5e8"
      },
      "id": "i8fXScxWAVIk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence[ZH→DX] BLEU: BLEU = 46.23 65.1/46.2/41.5/36.7 (BP = 1.000 ratio = 1.000 hyp_len = 63 ref_len = 63)\n",
            "Sentence[ZH→DX] ChrF++: chrF2++ = 59.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_dx = Chinese\n",
        "ref_zh = dongxiang\n",
        "\n",
        "hyps_zh = translate4(src_dx, src_lang=\"zho_Hans\", tgt_lang=\"sou_Latn\",no_repeat_ngram_size=3, num_beams=5)\n",
        "bleu_zh, chrf_zh = eval_bleu_chrf(hyps_zh, ref_zh, tgt_is_zh=True)\n",
        "print(\"Word[ZH→DX] BLEU:\", bleu_zh)\n",
        "print(\"Word[ZH→DX] ChrF++:\", chrf_zh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqXOKa3fAmc-",
        "outputId": "4a762d5d-7afd-4c30-fd81-ad3a2b0a8cbd"
      },
      "id": "TqXOKa3fAmc-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word[ZH→DX] BLEU: BLEU = 8.97 46.0/17.3/4.9/1.7 (BP = 1.000 ratio = 1.000 hyp_len = 63 ref_len = 63)\n",
            "Word[ZH→DX] ChrF++: chrF2++ = 36.05\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
